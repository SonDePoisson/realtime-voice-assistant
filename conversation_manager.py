import logging

logger = logging.getLogger(__name__)

import threading
import queue
import time
from typing import Optional
from pathlib import Path

from stt_module import TranscriptionProcessor, USE_TURN_DETECTION
from tts_module import AudioProcessor
from llm_module import LLM
from turn_detection import TurnDetection
from text_context import TextContext


class Generation:
    """Ã‰tat d'une gÃ©nÃ©ration unique (une rÃ©ponse de l'assistant)"""

    def __init__(self, user_text: str):
        self.user_text = user_text
        self.assistant_text = ""
        self.text_queue = queue.Queue()  # Queue pour streaming LLM â†’ TTS
        self.llm_completed = False
        self.tts_started = False
        self.tts_completed = False
        self.timestamp = time.time()


class ConversationManager:
    """
    Orchestre la conversation en temps rÃ©el avec 3 composants principaux:
    - STT (Speech-to-Text) avec Whisper tiny
    - LLM (Large Language Model) avec Ollama llama3.2:3b
    - TTS (Text-to-Speech) avec Kokoro voix franÃ§aise

    Architecture simplifiÃ©e Ã  2 worker threads:
    - Thread LLM: gÃ©nÃ¨re les rÃ©ponses en streaming
    - Thread TTS: synthÃ©tise et joue l'audio
    """

    def __init__(
        self,
        llm_provider: str = "ollama",
        llm_model: str = "llama3.2:3b",
        tts_engine: str = "kokoro",
        system_prompt_file: str = "system_prompt.txt",
    ):
        """
        Initialise le gestionnaire de conversation.

        Args:
            llm_provider: Backend LLM ("ollama", "openai", "lmstudio")
            llm_model: Nom du modÃ¨le LLM
            tts_engine: Moteur TTS ("kokoro", "coqui", "orpheus")
            system_prompt_file: Chemin vers le fichier de prompt systÃ¨me
        """
        logger.info("ğŸš€ Initialisation du ConversationManager...")

        # Charger le prompt systÃ¨me
        self.system_prompt = self._load_system_prompt(system_prompt_file)

        # Initialiser les composants
        logger.info("ğŸ§  Initialisation du LLM...")
        self.llm = LLM(
            backend=llm_provider,
            model=llm_model,
            system_prompt=self.system_prompt,
            no_think=False,
        )

        logger.info("ğŸ‘„ Initialisation du TTS...")
        self.tts = AudioProcessor(engine=tts_engine)

        logger.info("ğŸ‘‚ Initialisation du STT...")
        self.stt = TranscriptionProcessor(
            recorder_config=None,  # Utilise la config par dÃ©faut
            source_language="fr",
        )

        # Connecter les callbacks STT
        self.stt.full_transcription_callback = self._on_user_input
        self.stt.on_recording_start = self._on_user_interrupt

        # Turn detection (si activÃ©)
        if USE_TURN_DETECTION:
            logger.info("ğŸ¯ Turn detection activÃ©")
            # Le turn detection est dÃ©jÃ  gÃ©rÃ© dans TranscriptionProcessor

        # Ã‰tat de la conversation
        self.current_generation: Optional[Generation] = None
        self.history = []  # Historique des messages pour le LLM

        # Events de synchronisation
        self.abort_event = threading.Event()
        self.new_input_event = threading.Event()
        self.shutdown_event = threading.Event()

        # Worker threads
        self.llm_thread = None
        self.tts_thread = None

        logger.info("âœ… ConversationManager initialisÃ©")

    def _load_system_prompt(self, filepath: str) -> str:
        """Charge le prompt systÃ¨me depuis un fichier"""
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                prompt = f.read().strip()
                logger.info(f"ğŸ“„ Prompt systÃ¨me chargÃ© depuis {filepath}")
                return prompt
        except FileNotFoundError:
            logger.warning(
                f"âš ï¸ Fichier {filepath} introuvable, utilisation du prompt par dÃ©faut"
            )
            return "Tu es un assistant vocal franÃ§ais serviable et amical."

    def _start_workers(self):
        """DÃ©marre les threads workers"""
        logger.info("â–¶ï¸ DÃ©marrage des workers...")

        self.llm_thread = threading.Thread(
            target=self._llm_worker, name="LLM-Worker", daemon=True
        )
        self.llm_thread.start()

        self.tts_thread = threading.Thread(
            target=self._tts_worker, name="TTS-Worker", daemon=True
        )
        self.tts_thread.start()

        logger.info("âœ… Workers dÃ©marrÃ©s")

    def _on_user_input(self, text: str):
        """
        Callback appelÃ© quand le STT a finalisÃ© une transcription.

        Args:
            text: Texte transcrit de l'utilisateur
        """
        if not text or not text.strip():
            return

        text = text.strip()
        logger.info(f"ğŸ‘¤ Utilisateur: {text}")

        # Interrompre la gÃ©nÃ©ration en cours si elle existe
        self._abort_current()

        # CrÃ©er une nouvelle gÃ©nÃ©ration
        self.current_generation = Generation(text)

        # Ajouter Ã  l'historique
        self.history.append({"role": "user", "content": text})

        # Signaler le worker LLM
        self.new_input_event.set()

    def _on_user_interrupt(self):
        """Callback appelÃ© quand l'utilisateur commence Ã  parler (interruption)"""
        logger.info("ğŸ›‘ Interruption dÃ©tectÃ©e")
        self._abort_current()

    def _llm_worker(self):
        """
        Thread worker: gÃ©nÃ¨re les rÃ©ponses du LLM en streaming.

        Lit les nouvelles entrÃ©es utilisateur et gÃ©nÃ¨re les rÃ©ponses LLM,
        en plaÃ§ant chaque chunk de texte dans la queue pour le TTS.
        """
        logger.info("ğŸ§  LLM Worker dÃ©marrÃ©")

        while not self.shutdown_event.is_set():
            # Attendre un nouvel input avec timeout
            triggered = self.new_input_event.wait(timeout=1.0)

            if not triggered:
                continue

            self.new_input_event.clear()

            gen = self.current_generation
            if not gen:
                continue

            logger.info("ğŸ§  GÃ©nÃ©ration LLM en cours...")

            try:
                # GÃ©nÃ©rer la rÃ©ponse en streaming
                for chunk in self.llm.generate(
                    text=gen.user_text,
                    history=self.history[:-1],  # Historique sans le message actuel
                    use_system_prompt=True,
                ):
                    # VÃ©rifier si on doit arrÃªter
                    if self.abort_event.is_set():
                        logger.info("ğŸ§  GÃ©nÃ©ration LLM annulÃ©e")
                        break

                    # Ajouter le chunk Ã  la queue pour TTS
                    gen.text_queue.put(chunk)
                    gen.assistant_text += chunk

                # Marquer la gÃ©nÃ©ration LLM comme terminÃ©e
                if not self.abort_event.is_set():
                    gen.llm_completed = True
                    gen.text_queue.put(None)  # Signal de fin

                    # Ajouter la rÃ©ponse complÃ¨te Ã  l'historique
                    self.history.append(
                        {"role": "assistant", "content": gen.assistant_text}
                    )

                    logger.info(f"ğŸ¤– Assistant: {gen.assistant_text}")

                    # Limiter la taille de l'historique
                    if len(self.history) > 20:
                        self.history = self.history[-20:]

            except Exception as e:
                logger.error(f"ğŸ§ âŒ Erreur LLM: {e}", exc_info=True)
                gen.text_queue.put(None)  # Signal de fin en cas d'erreur

    def _tts_worker(self):
        """
        Thread worker: synthÃ©tise et joue l'audio Ã  partir des chunks de texte.

        Attend que des chunks de texte soient disponibles dans la queue,
        puis les synthÃ©tise en audio et les joue sur les haut-parleurs.
        """
        logger.info("ğŸ‘„ TTS Worker dÃ©marrÃ©")

        while not self.shutdown_event.is_set():
            time.sleep(0.01)  # Petite pause pour Ã©viter de saturer le CPU

            gen = self.current_generation
            if not gen or gen.tts_started:
                continue

            # Attendre qu'au moins un chunk soit disponible
            if gen.text_queue.empty():
                continue

            gen.tts_started = True
            logger.info("ğŸ‘„ SynthÃ¨se TTS dÃ©marrÃ©e...")

            # GÃ©nÃ©rateur qui consomme la queue
            def text_chunks():
                while True:
                    try:
                        chunk = gen.text_queue.get(timeout=0.1)
                        if chunk is None:
                            break
                        if self.abort_event.is_set():
                            break
                        yield chunk
                    except queue.Empty:
                        # VÃ©rifier si le LLM a terminÃ©
                        if gen.llm_completed:
                            break
                        continue

            try:
                # SynthÃ©tiser et jouer l'audio
                # La mÃ©thode synthesize_generator() va streamer les chunks
                completed = self.tts.synthesize_generator(
                    text_chunks(),
                    audio_chunks_queue=None,  # Pas de queue externe, lecture directe
                    abort_event=self.abort_event,
                )

                if completed and not self.abort_event.is_set():
                    gen.tts_completed = True
                    logger.info("ğŸ‘„âœ… SynthÃ¨se TTS terminÃ©e")
                else:
                    logger.info("ğŸ‘„ğŸ›‘ SynthÃ¨se TTS interrompue")

            except Exception as e:
                logger.error(f"ğŸ‘„âŒ Erreur TTS: {e}", exc_info=True)

    def _abort_current(self):
        """Interrompt la gÃ©nÃ©ration en cours"""
        if not self.current_generation:
            return

        logger.info("ğŸ›‘ Interruption de la gÃ©nÃ©ration en cours...")

        # Signaler l'interruption
        self.abort_event.set()

        # Annuler la gÃ©nÃ©ration LLM
        try:
            self.llm.cancel_generation()
        except Exception as e:
            logger.error(f"Erreur lors de l'annulation LLM: {e}")

        # ArrÃªter le TTS
        try:
            self.tts.stop()
        except Exception as e:
            logger.error(f"Erreur lors de l'arrÃªt TTS: {e}")

        # Vider la queue de texte
        while not self.current_generation.text_queue.empty():
            try:
                self.current_generation.text_queue.get_nowait()
            except queue.Empty:
                break

        # Petite pause pour laisser les workers rÃ©agir
        time.sleep(0.1)

        # Clear l'event d'interruption
        self.abort_event.clear()

        logger.info("âœ… Interruption complÃ¨te")

    def start(self):
        """DÃ©marre le systÃ¨me de conversation"""
        logger.info("ğŸ™ï¸ DÃ©marrage de l'assistant vocal...")

        # DÃ©marrer les workers
        self._start_workers()

        # DÃ©marrer le STT (Ã©coute du microphone)
        logger.info("ğŸ‘‚ DÃ©marrage de l'Ã©coute...")
        # Le STT dÃ©marre automatiquement dans TranscriptionProcessor

        logger.info("âœ… Assistant vocal prÃªt!")

    def shutdown(self):
        """ArrÃªte proprement le systÃ¨me"""
        logger.info("ğŸ›‘ ArrÃªt de l'assistant...")

        # Signaler l'arrÃªt
        self.shutdown_event.set()

        # ArrÃªter les composants
        try:
            if self.stt:
                self.stt.close()
        except Exception as e:
            logger.error(f"Erreur lors de l'arrÃªt STT: {e}")

        try:
            if self.tts:
                self.tts.stop()
        except Exception as e:
            logger.error(f"Erreur lors de l'arrÃªt TTS: {e}")

        # Attendre les threads
        if self.llm_thread:
            self.llm_thread.join(timeout=2.0)
        if self.tts_thread:
            self.tts_thread.join(timeout=2.0)

        logger.info("âœ… Assistant arrÃªtÃ©")
